---
title: "Beta Regression"
subtitle: "Team Bee (Beta Regressionists) (Advisor: Dr. Seals)"
author: "Anaite Montes Bu, Travis Keep"
editor: source
format: 
  revealjs: 
    toc: true
    theme: dark
    self-contained: true
    css: custom.css
    html-math-method: katex
bibliography: references1.bib
csl: ieee.csl
execute:
  warning: false
  message: false
  error: false
---

## Introduction

-   Regression analysis is a statistical tool used to explore relationships between variables.

-   Beta Regression: When the dependent variable is a ratio or percentage.

## Shortfalls of Normal Regression

- Assumes dependent variable is normally distributed
- Assumes variance is constant throughout

Generally not true with ratios or percentages

## When dependent variable lies in (0, 1)

- True of ratios or percents such as test scores
- Variance typically less near the extremes e.g 0 or 1.

## Reading Skills data set

- Reading skills based on IQ and if student has dyslexia N=44
- Normal regression shows IQ alone not significant in predicting reading score
- Beta regression shows IQ alone is significant in predicting reading score
[@smithson2006]

## Beta distribution

The PDF of random variable with a beta distribution is as follows.

$$
f(y) = \begin{cases} 
      \frac{y^{\alpha-1}(1-y)^{\beta-1}}{B(\alpha,\beta)}, & 0 \le y \le 1 \\
      0, & \text{elsewhere}
\end{cases}
$$
Where $B(\alpha,\beta) = \int_0^1 y^{\alpha-1}(1-y)^{\beta-1} \ dy = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha+\beta)}.$

$\alpha$ and $\beta$ are the shape variables where $\alpha > 0 \quad \beta > 0$.
[@wackerly2002]

## Beta Distribution Mean and Variance

$$
E[Y] = \mu = \frac{\alpha}{\alpha+\beta} \ \ \ \text{and} \ \ \ V[Y] = \sigma^2  = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
$$
[@wackerly2002]

## Introduction of $\mu$ and $\phi$

For beta regression, it is useful to introduce the following

$$
\mu = \frac{\alpha}{\alpha+\beta} \\
\phi = \alpha + \beta
$$
$\mu$ is the mean of the beta regression while the higher the $\phi$ the less the variance or the less spread out the PDF function is. [@ferrari2004]

## Revised Beta Distribution

$$
f(y; \mu, \phi) = \frac{\Gamma(\phi)}{\Gamma(\mu \phi) \Gamma((1 - \mu)\phi)} y^{\mu\phi - 1}(1 - y)^{(1 - \mu)\phi - 1}, \quad 0 < y < 1
$$
Where:\
- **(μ)** is the mean, **(ϕ)** is the precision (inverse of the variance), **(Γ)** is the gamma function.

## Beta Distribution Variance

$$
\text{Var}(Y) = \frac{\mu(1 - \mu)}{1 + \phi}
$$
When $\mu$ is near the extremes, 0 or 1, variance drops. [@Ferrari2004]

## Extended Beta Regression

**Bias** **Correction/Reduction**
- **Type** **of** **Estimator:**

- ML (Maximum Likelihood): Standard method, useful but may yield biased estimates in certain conditions.[@Grun2012]
- BC (Bias-Corrected): Adjusts estimates to correct for bias, providing more reliable parameter values.
- BR (Bias-Reduced): Shrinks estimates towards a central value, which can improve predictive performance.

## **Bias** **Correction/Reduction**

```{r}
library(betareg)
library(readr)
suicide_dataset <- read_csv("master.csv")

max_suicide_rate <- max(suicide_dataset$`suicides/100k pop`, na.rm = TRUE)
suicide_dataset$suicide_rate <- ifelse(suicide_dataset$`suicides/100k pop` == 0, 0.001,
                                        ifelse(suicide_dataset$`suicides/100k pop` == max_suicide_rate, 0.999,
                                               suicide_dataset$`suicides/100k pop` / max_suicide_rate))

# Rename variables
names(suicide_dataset)[names(suicide_dataset) == "HDI for year"] <- "HDI_year"
names(suicide_dataset)[names(suicide_dataset) == "gdp_per_capita ($)"] <- "GDP_capita"

# Remove rows with NA values
suicide_dataset <- na.omit(suicide_dataset)

# Mean model
m1 <- suicide_rate ~ HDI_year + GDP_capita + sex + age + generation | HDI_year + GDP_capita

# Fit beta regression models
suicide_ml <- betareg(m1, data = suicide_dataset, type = "ML")
suicide_bc <- betareg(m1, data = suicide_dataset, type = "BC")
suicide_br <- betareg(m1, data = suicide_dataset, type = "BR")

summary(suicide_bc)
```


## Beta Regression Trees

-   This extension uses recursive partitioning to model data that might exhibit subgroup-specific relationships. 
-   It builds decision trees by splitting data into different subgroups based on the instability of model parameters across partitioning variables.



```{r}
library(partykit)
set.seed(123)

# Randomly sample 20% of the data
sample_indices <- sample(seq_len(nrow(suicide_dataset)), size = 0.2 * nrow(suicide_dataset))
suicide_sample <- suicide_dataset[sample_indices, ]

# Fit the model on the sample
suicide_tree <- betatree(suicide_rate ~ HDI_year, 
                         data = suicide_sample, 
                         minsplit = 20,
                         control = betatree.control(maxdepth = 3))

# Plot the resulting tree
plot(suicide_tree)
```

## Diagnostic Plots
The package **betareg** allows users to perform both fixed and variable dispersion beta regression. The model is based on the beta distribution, using a parameterization with the mean and precision[@Zeileis2004].

```{r}
par(mfrow = c(2, 2))
plot(suicide_bc)
```

## Reading Skills Test Data Set
- 44 children
- 19 dyslexic / 25 normal
- Test scores range from 0.0 to 1.0

## Reading Skills Data Set Regressors
- IQ (Z-score) 
  - Min -1.745
  - Median -0.122
  - Max 1.856
- Dyslexia
  - Yes
  - No
  
## Reading Skills Dataset Tweaking
- Dyslexia
  - No -> 0.0
  - Yes -> 1.0
- Reading Score
  - 1.0 -> 0.99 

Remember dependent variable is in open interval (0, 1)

## Beta Regression Fitting
~~~
betareg(
  formula = accuracy ~ dcode * iq,
  data = ReadingSkillsModel,
  type = "BC",
)
~~~

- Phi modeled as constant for higher psuedo $R^2$
- BC is Bias Correction

## General Linear Regression
~~~
glm(
  formula = accuracy ~ dcode * iq,
  family = gaussian(link = "logit"), 
  data = ReadingSkillsModel,
)
~~~

- logit maps (0, 1) to $\mathbb{R}$

## Data Cleaning

A point is extreme and removed if

- Cooks Distance > 4 / N OR
- Leverage > 2P / N. P is rank of model OR
- Residual > 3 standard deviations

## Results for Normal Children

```{r}
library(betareg)
library(dplyr)
library(magrittr)
library(ggplot2)

data("ReadingSkills")

ReadingSkillsModel <- ReadingSkills %>% mutate(dcode=ifelse(dyslexia == "no", 0.0, 1.0))

model <- betareg(accuracy ~ dcode*iq, data=ReadingSkillsModel, type="BC")

#Identify influential points
cooks_dist <- cooks.distance(model)
leverage <- hatvalues(model)
residuals <- residuals(model, type = "pearson")

# Set thresholds
cooks_threshold <- 4 / nrow(ReadingSkillsModel)
leverage_threshold <- 2 * (length(coef(model)) / nrow(ReadingSkillsModel))
residual_threshold <- 3

#Identify extreme points
extreme_points <- which(cooks_dist > cooks_threshold |
                        leverage > leverage_threshold |
                        abs(residuals) > residual_threshold)

ReadingSkillsModel.cleaned <- ReadingSkillsModel[-extreme_points, ]

model <- betareg(accuracy ~ dcode*iq, data=ReadingSkillsModel.cleaned, type="BC")
linmodel <- glm(accuracy ~ dcode*iq, family=gaussian(link="logit"), data=ReadingSkillsModel.cleaned)

dyslexiaModel <- ReadingSkillsModel.cleaned %>% filter(dyslexia == "yes")
normalModel <- ReadingSkillsModel.cleaned %>% filter(dyslexia == "no")

ggplot() + geom_point(aes(x=normalModel$iq, y=normalModel$accuracy), color="red") +
geom_line(aes(x=normalModel$iq, y=predict(model, newdata=normalModel), color="beta")) +
geom_line(aes(x=normalModel$iq, y=plogis(predict(linmodel, newdata=normalModel)), color="glm")) +
xlab("IQ (Z-score)") + ylab("Score") + ggtitle("Score by IQ for normal students") +
scale_color_manual(name="Regression Model", breaks=c("beta", "glm"), values=c("beta"="red", "glm"="green"))
```


## Results for Dyslexic Children

```{r}
ggplot() + geom_point(aes(x=dyslexiaModel$iq, y=dyslexiaModel$accuracy), color="blue") + 
geom_line(aes(x=dyslexiaModel$iq, y=predict(model, newdata=dyslexiaModel), color="beta")) +
geom_line(aes(x=dyslexiaModel$iq, y=plogis(predict(linmodel, newdata=dyslexiaModel)), color="glm")) +
xlab("IQ (Z-score)") + ylab("Score") + ggtitle("Score by IQ for dyslexic students") +
scale_color_manual(name="Regression Model", breaks=c("beta", "glm"), values=c("beta"="blue", "glm"="green"))
```

## Table 2

```{r}
model.summary <- summary(model)
linmodel.summary <- summary(linmodel)

model.coefs <- model.summary$coefficients$mean
linmodel.coefs <- linmodel.summary$coefficients
```

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:28px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:28px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-pb0m{border-color:inherit;text-align:center;vertical-align:bottom}
.tg .tg-kg9c{background-color:#D9D9D9;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}
.tg .tg-jkyp{border-color:inherit;text-align:right;vertical-align:bottom}
</style>
<table class="tg"><thead>
  <tr>
    <th class="tg-kg9c" colspan="7" rowspan="2">Table 2: Association of Reading Skills Score with IQ and presence of Dyslexia</th>
  </tr>
  <tr>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-c3ow" rowspan="2">Variable</td>
    <td class="tg-pb0m" colspan="3">Beta Regression</td>
    <td class="tg-pb0m" colspan="3">General Linear Regression</td>
  </tr>
  <tr>
    <td class="tg-pb0m">β</td>
    <td class="tg-pb0m">SE</td>
    <td class="tg-pb0m">p</td>
    <td class="tg-pb0m">β</td>
    <td class="tg-pb0m">SE</td>
    <td class="tg-pb0m">p</td>
  </tr>
  <tr>
    <td class="tg-za14">Dyslexia</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['dcode', 1])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['dcode', 2])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['dcode', 4])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['dcode', 1])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['dcode', 2])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['dcode', 4])`</td>
  </tr>
  <tr>
    <td class="tg-za14">IQ (Z-score)</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['iq', 1])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['iq', 2])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['iq', 4])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['iq', 1])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['iq', 2])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['iq', 4])`</td>
  </tr>
  <tr>
    <td class="tg-za14">Dyslexia:iq</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['dcode:iq', 1])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['dcode:iq', 2])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['dcode:iq', 4])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['dcode:iq', 1])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['dcode:iq', 2])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['dcode:iq', 4])`</td>
  </tr>
  <tr>
  </tr>
</tbody></table>
<br><br>

## Dyslexia's effect on scores

A child's odds of answering a reading skills question correctly decreases by a
factor of $e^{2.277}$ if they are dyslexic assuming normal IQ.

## IQ's effect on scores

If a normal child's IQ increases by 1 standard deviation, their odds of
answering a reading skills question correctly increases by a factor of
$e^{0.3205}$

## IQ's effect on scores cont'd

If a dyslexic child's IQ increases by 1 standard deviation, their odds of
answering a reading skills question correctly decreases by a factor of
$e^{0.0532}$

0.0532 = 0.3737 - 0.3205

## Conclusion

-   Effective for proportion data, Ideal for modeling data bounded in the (0, 1) range.
-   Models both mean and precision, managing boundary cases and latent heterogeneity.
-   Bias correction and beta regression trees expand its capabilities.
-   The betareg package in R offers a powerful, flexible framework for analysts.

## References


